This Work
---------

This is the result of a simple exercise.  The source mixing matrix is a
simple 2x2 matrix with the following values:

    m[0,0] = 0.5
    m[0,1] = 0.25
    m[1,0] = -0.1
    m[1,1] = -0.3

Training and testing data is generated by creating a random input vector, 
of size 1x2, with each element in the range -1,1.  80% of the data
is used as training data and 20% is used to measure the performance metrics.

A tensorflow model of the same dimensions as the original matrix is
created and initialized with the default initialization, and the linear
activation function.  The model is fit to the training data, over 10 epochs,
using the mean squared error loss function, the Adam optimizer.

Next the fit model is evaluated against the test data, measuring the
loss function, the mean absolute error (MAE), and the MAE of the
original matrix compared to the learned matrix in the model.

This process is carried out 10 times for each of the sample data sizes:
[ 10, 20, 40, 100, 200, 400, 1000, 2000, 4000, 10000 ].

The results are displayed as a scatter plot in FitQualityVsDataSize.pdf.

It appears that close to 10,000 data samples are needed for the
learned matrix to stabilize with respect to the original matrix.

Other hand-built original matrices. 
-----------------------------------

Built a second hand selected 2x2 matrix.  Saw similar
learning profile as the original hand built one.


Random original matrices. 
-------------------------

Generated random matrices, of size 2x2, and see similar loss and
error profiles to the custom hand built 2x2 matrices.  Only using
coefficients in the range $-1.0 \le m_{ij} \le 1.0$.


Added data series/sequences for training and testing 
-----------------------------------------------------

Data sequences longer than 1.  (x0,y0)...(xn,yn) instead of a bunch of (x0,y0),(x1,y1) 
pairs were added.  The fit appeared to be the same for the same total number 
of data points. This seems to indicate that a sequence of data points is not
necessary for training.  The data points can be spread out.

Future Work
-----------

- Larger original matrices.
- Study number of epochs.
- Other loss functions?
- Other metrics?  Is there a better way to measure how well
  the learned matrix matches the original?
- Regularization techniques for sparse matrices.
- Other?
